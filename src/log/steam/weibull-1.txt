nohup: ignoring input
Namespace(batch_size=512, dataset='steam', decay_step=100, description='Diffu_norm_score', device='cuda', diffusion_steps=32, diversity_measure=False, dropout=0.1, emb_dropout=0.3, epoch_time_avg=False, epochs=500, eval_interval=20, gamma=0.1, hidden_act='gelu', hidden_size=128, lambda_uncertainty=0.001, log_file='log/', long_head=False, loss_lambda=0.001, lr=0.001, max_len=50, metric_ks=[5, 10, 20], momentum=None, noise_schedule='trunc_lin', num_blocks=4, num_gpu=1, optimizer='Adam', patience=5, random_seed=1997, rescale_timesteps=True, schedule_sampler_name='lossaware', weight_decay=0)
Epoch: 0
[0/5166] Loss: 40.3617
[1034/5166] Loss: 8.2869
[2068/5166] Loss: 7.9943
[3102/5166] Loss: 7.8088
[4136/5166] Loss: 7.8561
loss in epoch 0: 7.878415107727051
Epoch: 1
[0/5166] Loss: 7.8523
[1034/5166] Loss: 7.8608
[2068/5166] Loss: 7.7940
[3102/5166] Loss: 7.8401
[4136/5166] Loss: 7.7863
loss in epoch 1: 7.674903869628906
Epoch: 2
[0/5166] Loss: 7.7818
[1034/5166] Loss: 7.9232
[2068/5166] Loss: 7.7655
[3102/5166] Loss: 7.8833
[4136/5166] Loss: 7.7648
loss in epoch 2: 7.888707160949707
Epoch: 3
[0/5166] Loss: 7.8375
[1034/5166] Loss: 7.8846
[2068/5166] Loss: 7.8151
[3102/5166] Loss: 7.8460
[4136/5166] Loss: 7.7473
loss in epoch 3: 7.652135848999023
Epoch: 4
[0/5166] Loss: 7.8091
[1034/5166] Loss: 7.8260
[2068/5166] Loss: 7.8575
[3102/5166] Loss: 7.7306
[4136/5166] Loss: 7.8431
loss in epoch 4: 7.868772506713867
Epoch: 5
[0/5166] Loss: 7.7870
[1034/5166] Loss: 7.6876
[2068/5166] Loss: 7.7421
[3102/5166] Loss: 7.8412
[4136/5166] Loss: 7.7853
loss in epoch 5: 7.8399658203125
Epoch: 6
[0/5166] Loss: 7.7532
[1034/5166] Loss: 7.9632
[2068/5166] Loss: 7.7924
[3102/5166] Loss: 7.6899
[4136/5166] Loss: 7.6583
loss in epoch 6: 7.77946662902832
Epoch: 7
[0/5166] Loss: 7.7553
[1034/5166] Loss: 7.7195
[2068/5166] Loss: 7.7454
[3102/5166] Loss: 7.7641
[4136/5166] Loss: 7.8140
loss in epoch 7: 7.732553482055664
Epoch: 8
[0/5166] Loss: 7.7815
[1034/5166] Loss: 7.8003
[2068/5166] Loss: 7.7752
[3102/5166] Loss: 7.7451
[4136/5166] Loss: 7.6374
loss in epoch 8: 7.838420391082764
Epoch: 9
[0/5166] Loss: 7.6907
[1034/5166] Loss: 7.6780
[2068/5166] Loss: 7.7175
[3102/5166] Loss: 7.6922
[4136/5166] Loss: 7.7477
loss in epoch 9: 7.914356708526611
Epoch: 10
[0/5166] Loss: 7.6412
[1034/5166] Loss: 7.5926
[2068/5166] Loss: 7.6587
[3102/5166] Loss: 7.6849
[4136/5166] Loss: 7.5055
loss in epoch 10: 7.825697898864746
Epoch: 11
[0/5166] Loss: 7.5966
[1034/5166] Loss: 7.6383
[2068/5166] Loss: 7.7485
[3102/5166] Loss: 7.6490
[4136/5166] Loss: 7.4998
loss in epoch 11: 7.430001258850098
Epoch: 12
[0/5166] Loss: 7.6139
[1034/5166] Loss: 7.4944
[2068/5166] Loss: 7.5691
[3102/5166] Loss: 7.5470
[4136/5166] Loss: 7.6304
loss in epoch 12: 7.62026834487915
Epoch: 13
[0/5166] Loss: 7.5048
[1034/5166] Loss: 7.3908
[2068/5166] Loss: 7.4861
[3102/5166] Loss: 7.6016
[4136/5166] Loss: 7.6162
loss in epoch 13: 7.561985492706299
Epoch: 14
[0/5166] Loss: 7.6349
[1034/5166] Loss: 7.6249
[2068/5166] Loss: 7.5593
[3102/5166] Loss: 7.6411
[4136/5166] Loss: 7.5627
loss in epoch 14: 7.543050289154053
Epoch: 15
[0/5166] Loss: 7.5614
[1034/5166] Loss: 7.4840
[2068/5166] Loss: 7.3645
[3102/5166] Loss: 7.4895
[4136/5166] Loss: 7.4879
loss in epoch 15: 7.5389580726623535
Epoch: 16
[0/5166] Loss: 7.4527
[1034/5166] Loss: 7.5864
[2068/5166] Loss: 7.4697
[3102/5166] Loss: 7.5332
[4136/5166] Loss: 7.4105
loss in epoch 16: 7.696460723876953
Epoch: 17
[0/5166] Loss: 7.4358
[1034/5166] Loss: 7.4418
[2068/5166] Loss: 7.4754
[3102/5166] Loss: 7.6730
[4136/5166] Loss: 7.5306
loss in epoch 17: 7.701749801635742
Epoch: 18
[0/5166] Loss: 7.5013
[1034/5166] Loss: 7.4471
[2068/5166] Loss: 7.4109
[3102/5166] Loss: 7.4228
[4136/5166] Loss: 7.4527
loss in epoch 18: 7.159847736358643
Epoch: 19
[0/5166] Loss: 7.3857
[1034/5166] Loss: 7.3173
[2068/5166] Loss: 7.3523
[3102/5166] Loss: 7.4665
[4136/5166] Loss: 7.3632
loss in epoch 19: 7.4455342292785645
Epoch: 20
[0/5166] Loss: 7.4376
[1034/5166] Loss: 7.6796
[2068/5166] Loss: 7.5234
[3102/5166] Loss: 7.4239
[4136/5166] Loss: 7.4195
loss in epoch 20: 7.2614426612854
start predicting:  2024-12-01 10:35:56.024617
{'Best_HR@5': 0.5538, 'Best_NDCG@5': 0.3667, 'Best_HR@10': 0.9127, 'Best_NDCG@10': 0.482, 'Best_HR@20': 1.5153, 'Best_NDCG@20': 0.6328}
{'Best_epoch_HR@5': 20, 'Best_epoch_NDCG@5': 20, 'Best_epoch_HR@10': 20, 'Best_epoch_NDCG@10': 20, 'Best_epoch_HR@20': 20, 'Best_epoch_NDCG@20': 20}
Epoch: 21
[0/5166] Loss: 7.5152
[1034/5166] Loss: 7.3780
[2068/5166] Loss: 7.2830
[3102/5166] Loss: 7.3077
[4136/5166] Loss: 7.4263
loss in epoch 21: 7.387783050537109
Epoch: 22
[0/5166] Loss: 7.2730
[1034/5166] Loss: 7.3560
[2068/5166] Loss: 7.5393
[3102/5166] Loss: 7.1970
[4136/5166] Loss: 7.2119
loss in epoch 22: 7.372393608093262
Epoch: 23
[0/5166] Loss: 7.2052
[1034/5166] Loss: 7.3514
[2068/5166] Loss: 7.2717
[3102/5166] Loss: 7.4174
[4136/5166] Loss: 7.3514
loss in epoch 23: 7.322747707366943
Epoch: 24
[0/5166] Loss: 6.9856
[1034/5166] Loss: 7.3951
[2068/5166] Loss: 7.3052
[3102/5166] Loss: 7.2049
[4136/5166] Loss: 7.3063
loss in epoch 24: 7.235980033874512
Epoch: 25
[0/5166] Loss: 7.0766
[1034/5166] Loss: 7.2098
[2068/5166] Loss: 7.1944
[3102/5166] Loss: 7.2301
[4136/5166] Loss: 7.2295
loss in epoch 25: 7.3914031982421875
Epoch: 26
[0/5166] Loss: 7.3088
[1034/5166] Loss: 7.2562
[2068/5166] Loss: 7.3075
[3102/5166] Loss: 7.0028
[4136/5166] Loss: 7.1532
loss in epoch 26: 7.29750394821167
Epoch: 27
[0/5166] Loss: 7.0143
[1034/5166] Loss: 7.0267
[2068/5166] Loss: 7.2581
[3102/5166] Loss: 7.1676
[4136/5166] Loss: 7.3202
loss in epoch 27: 7.468912601470947
Epoch: 28
[0/5166] Loss: 7.0645
[1034/5166] Loss: 7.1940
[2068/5166] Loss: 7.0117
[3102/5166] Loss: 7.1771
[4136/5166] Loss: 7.1246
loss in epoch 28: 7.357171058654785
Epoch: 29
[0/5166] Loss: 7.1822
[1034/5166] Loss: 7.3202
[2068/5166] Loss: 7.1979
[3102/5166] Loss: 7.1799
[4136/5166] Loss: 7.1645
loss in epoch 29: 7.239055156707764
Epoch: 30
[0/5166] Loss: 7.1496
[1034/5166] Loss: 7.0316
[2068/5166] Loss: 7.1102
[3102/5166] Loss: 7.1087
[4136/5166] Loss: 7.1600
loss in epoch 30: 7.007713317871094
Epoch: 31
[0/5166] Loss: 7.0774
[1034/5166] Loss: 7.0305
[2068/5166] Loss: 6.9655
[3102/5166] Loss: 7.0997
[4136/5166] Loss: 7.2737
loss in epoch 31: 7.244309425354004
Epoch: 32
[0/5166] Loss: 7.1107
[1034/5166] Loss: 7.0135
[2068/5166] Loss: 6.9800
[3102/5166] Loss: 7.2437
[4136/5166] Loss: 7.1834
loss in epoch 32: 7.036472797393799
Epoch: 33
[0/5166] Loss: 7.0955
[1034/5166] Loss: 6.7944
[2068/5166] Loss: 6.9661
[3102/5166] Loss: 7.0347
[4136/5166] Loss: 6.9760
loss in epoch 33: 6.944149494171143
Epoch: 34
[0/5166] Loss: 7.0137
[1034/5166] Loss: 7.2182
[2068/5166] Loss: 6.9324
[3102/5166] Loss: 7.0569
[4136/5166] Loss: 6.8116
loss in epoch 34: 7.038793087005615
Epoch: 35
[0/5166] Loss: 6.9427
[1034/5166] Loss: 6.8058
[2068/5166] Loss: 6.8889
[3102/5166] Loss: 6.9540
[4136/5166] Loss: 7.1229
loss in epoch 35: 7.249832630157471
Epoch: 36
[0/5166] Loss: 7.0084
[1034/5166] Loss: 7.0115
[2068/5166] Loss: 7.0051
[3102/5166] Loss: 6.9961
[4136/5166] Loss: 7.0511
loss in epoch 36: 6.615230560302734
Epoch: 37
[0/5166] Loss: 6.9761
[1034/5166] Loss: 7.0154
[2068/5166] Loss: 7.0136
[3102/5166] Loss: 7.0638
[4136/5166] Loss: 7.0317
loss in epoch 37: 6.63290548324585
Epoch: 38
[0/5166] Loss: 6.6766
[1034/5166] Loss: 6.9041
[2068/5166] Loss: 6.9005
[3102/5166] Loss: 6.9017
[4136/5166] Loss: 6.9987
loss in epoch 38: 6.82288122177124
Epoch: 39
[0/5166] Loss: 7.0719
[1034/5166] Loss: 6.8433
[2068/5166] Loss: 6.8717
[3102/5166] Loss: 6.9981
[4136/5166] Loss: 6.7433
loss in epoch 39: 6.751141548156738
Epoch: 40
[0/5166] Loss: 6.8078
[1034/5166] Loss: 7.0992
[2068/5166] Loss: 6.7288
[3102/5166] Loss: 7.1558
[4136/5166] Loss: 6.8712
loss in epoch 40: 6.905282497406006
start predicting:  2024-12-02 01:59:26.549419
Epoch: 41
[0/5166] Loss: 7.1264
[1034/5166] Loss: 6.9937
[2068/5166] Loss: 7.0271
[3102/5166] Loss: 6.9064
[4136/5166] Loss: 6.9154
loss in epoch 41: 6.725907325744629
Epoch: 42
[0/5166] Loss: 6.9270
[1034/5166] Loss: 6.8889
[2068/5166] Loss: 6.7441
[3102/5166] Loss: 7.0656
[4136/5166] Loss: 6.7158
loss in epoch 42: 6.64771842956543
Epoch: 43
[0/5166] Loss: 6.7652
[1034/5166] Loss: 6.8075
[2068/5166] Loss: 6.6542
[3102/5166] Loss: 6.8566
[4136/5166] Loss: 6.6414
loss in epoch 43: 6.866605758666992
Epoch: 44
[0/5166] Loss: 6.8337
[1034/5166] Loss: 6.6524
[2068/5166] Loss: 6.7234
[3102/5166] Loss: 6.8729
[4136/5166] Loss: 6.8258
loss in epoch 44: 6.863158702850342
Epoch: 45
[0/5166] Loss: 6.8496
[1034/5166] Loss: 6.8360
[2068/5166] Loss: 6.8389
[3102/5166] Loss: 7.0036
[4136/5166] Loss: 6.8235
loss in epoch 45: 6.604401588439941
Epoch: 46
[0/5166] Loss: 6.9082
[1034/5166] Loss: 7.0026
[2068/5166] Loss: 6.8395
[3102/5166] Loss: 6.8533
[4136/5166] Loss: 6.7400
loss in epoch 46: 7.347249984741211
Epoch: 47
[0/5166] Loss: 6.7759
[1034/5166] Loss: 6.8043
[2068/5166] Loss: 6.7023
[3102/5166] Loss: 6.8127
[4136/5166] Loss: 6.7833
loss in epoch 47: 6.9247355461120605
Epoch: 48
[0/5166] Loss: 6.7292
[1034/5166] Loss: 6.8237
[2068/5166] Loss: 6.8023
[3102/5166] Loss: 6.8209
[4136/5166] Loss: 6.6871
loss in epoch 48: 6.7718915939331055
Epoch: 49
[0/5166] Loss: 6.8310
[1034/5166] Loss: 6.9160
[2068/5166] Loss: 6.7100
[3102/5166] Loss: 6.7422
[4136/5166] Loss: 6.6268
loss in epoch 49: 6.673920154571533
Epoch: 50
[0/5166] Loss: 6.6256
[1034/5166] Loss: 6.8317
[2068/5166] Loss: 6.7840
[3102/5166] Loss: 6.8470
[4136/5166] Loss: 6.8349
loss in epoch 50: 6.3383355140686035
Epoch: 51
[0/5166] Loss: 6.6496
[1034/5166] Loss: 6.8101
[2068/5166] Loss: 6.6039
[3102/5166] Loss: 6.7314
[4136/5166] Loss: 6.7232
loss in epoch 51: 6.632514953613281
Epoch: 52
[0/5166] Loss: 6.7146
[1034/5166] Loss: 6.6541
[2068/5166] Loss: 6.7798
[3102/5166] Loss: 6.7797
[4136/5166] Loss: 6.6699
loss in epoch 52: 6.794010162353516
Epoch: 53
[0/5166] Loss: 6.9850
[1034/5166] Loss: 6.5696
[2068/5166] Loss: 6.7143
[3102/5166] Loss: 6.6888
[4136/5166] Loss: 6.6189
loss in epoch 53: 6.69880485534668
Epoch: 54
[0/5166] Loss: 6.6939
[1034/5166] Loss: 6.7200
[2068/5166] Loss: 6.8571
[3102/5166] Loss: 6.5267
[4136/5166] Loss: 6.6236
loss in epoch 54: 6.51975679397583
Epoch: 55
[0/5166] Loss: 6.6473
[1034/5166] Loss: 6.4760
[2068/5166] Loss: 6.7836
[3102/5166] Loss: 6.7462
[4136/5166] Loss: 6.8677
loss in epoch 55: 6.923192977905273
Epoch: 56
[0/5166] Loss: 6.5115
[1034/5166] Loss: 6.5217
[2068/5166] Loss: 6.5332
[3102/5166] Loss: 6.5523
[4136/5166] Loss: 6.5155
loss in epoch 56: 6.5987467765808105
Epoch: 57
[0/5166] Loss: 6.6132
[1034/5166] Loss: 6.5566
[2068/5166] Loss: 6.6635
[3102/5166] Loss: 6.8306
[4136/5166] Loss: 6.7308
loss in epoch 57: 6.674961566925049
Epoch: 58
[0/5166] Loss: 6.7176
[1034/5166] Loss: 6.6775
[2068/5166] Loss: 6.7338
[3102/5166] Loss: 6.4877
[4136/5166] Loss: 6.5740
loss in epoch 58: 6.598680019378662
Epoch: 59
[0/5166] Loss: 6.5196
[1034/5166] Loss: 6.5286
[2068/5166] Loss: 6.6385
[3102/5166] Loss: 6.6474
[4136/5166] Loss: 6.4532
loss in epoch 59: 6.670165538787842
Epoch: 60
[0/5166] Loss: 6.6017
[1034/5166] Loss: 6.6058
[2068/5166] Loss: 6.6184
[3102/5166] Loss: 6.7643
[4136/5166] Loss: 6.5945
loss in epoch 60: 6.517874717712402
start predicting:  2024-12-03 05:16:33.028355
Epoch: 61
[0/5166] Loss: 6.5609
[1034/5166] Loss: 6.5281
[2068/5166] Loss: 6.5651
[3102/5166] Loss: 6.4920
[4136/5166] Loss: 6.6366
loss in epoch 61: 6.700584411621094
Epoch: 62
[0/5166] Loss: 6.7379
[1034/5166] Loss: 6.4743
[2068/5166] Loss: 6.5687
[3102/5166] Loss: 6.6225
[4136/5166] Loss: 6.3282
loss in epoch 62: 6.719533920288086
Epoch: 63
[0/5166] Loss: 6.6560
[1034/5166] Loss: 6.4976
[2068/5166] Loss: 6.4346
[3102/5166] Loss: 6.5613
[4136/5166] Loss: 6.5968
loss in epoch 63: 6.7981181144714355
Epoch: 64
[0/5166] Loss: 6.6402
[1034/5166] Loss: 6.5277
[2068/5166] Loss: 6.4994
[3102/5166] Loss: 6.5680
[4136/5166] Loss: 6.6210
loss in epoch 64: 6.989715099334717
Epoch: 65
[0/5166] Loss: 6.2886
[1034/5166] Loss: 6.3976
[2068/5166] Loss: 6.3580
[3102/5166] Loss: 6.3502
[4136/5166] Loss: 6.6028
loss in epoch 65: 6.755734920501709
Epoch: 66
[0/5166] Loss: 6.5020
[1034/5166] Loss: 6.4386
[2068/5166] Loss: 6.5256
[3102/5166] Loss: 6.3821
[4136/5166] Loss: 6.4848
loss in epoch 66: 5.887670040130615
Epoch: 67
[0/5166] Loss: 6.2952
[1034/5166] Loss: 6.5390
[2068/5166] Loss: 6.5852
[3102/5166] Loss: 6.4638
[4136/5166] Loss: 6.5248
loss in epoch 67: 6.743592262268066
Epoch: 68
[0/5166] Loss: 6.5010
[1034/5166] Loss: 6.2314
[2068/5166] Loss: 6.4253
[3102/5166] Loss: 6.4974
[4136/5166] Loss: 6.4367
loss in epoch 68: 6.270941734313965
Epoch: 69
[0/5166] Loss: 6.4486
[1034/5166] Loss: 6.2706
[2068/5166] Loss: 6.2925
[3102/5166] Loss: 6.4791
[4136/5166] Loss: 6.5340
loss in epoch 69: 6.5332841873168945
Epoch: 70
[0/5166] Loss: 6.4353
[1034/5166] Loss: 6.3911
[2068/5166] Loss: 6.6247
[3102/5166] Loss: 6.2864
[4136/5166] Loss: 6.4087
loss in epoch 70: 6.830112457275391
Epoch: 71
[0/5166] Loss: 6.1345
[1034/5166] Loss: 6.5779
[2068/5166] Loss: 6.2409
[3102/5166] Loss: 6.3736
[4136/5166] Loss: 6.3056
loss in epoch 71: 5.840728282928467
Epoch: 72
[0/5166] Loss: 6.7150
[1034/5166] Loss: 6.4290
[2068/5166] Loss: 6.2716
[3102/5166] Loss: 6.1002
[4136/5166] Loss: 6.5214
loss in epoch 72: 6.411442756652832
Epoch: 73
[0/5166] Loss: 6.2959
[1034/5166] Loss: 6.4114
[2068/5166] Loss: 6.6523
[3102/5166] Loss: 6.4875
[4136/5166] Loss: 6.3804
loss in epoch 73: 6.037055969238281
Epoch: 74
[0/5166] Loss: 6.4477
[1034/5166] Loss: 6.2740
[2068/5166] Loss: 6.3073
[3102/5166] Loss: 6.2409
[4136/5166] Loss: 6.2381
loss in epoch 74: 6.0593037605285645
Epoch: 75
[0/5166] Loss: 6.1747
[1034/5166] Loss: 6.3021
[2068/5166] Loss: 6.2528
[3102/5166] Loss: 6.4147
[4136/5166] Loss: 6.2884
loss in epoch 75: 6.784235000610352
Epoch: 76
[0/5166] Loss: 6.1468
[1034/5166] Loss: 6.3543
[2068/5166] Loss: 6.3833
[3102/5166] Loss: 6.4446
[4136/5166] Loss: 6.3284
loss in epoch 76: 6.005405426025391
Epoch: 77
[0/5166] Loss: 6.5460
[1034/5166] Loss: 6.1930
[2068/5166] Loss: 6.5787
[3102/5166] Loss: 6.3058
[4136/5166] Loss: 6.3985
loss in epoch 77: 6.821986198425293
Epoch: 78
[0/5166] Loss: 6.2402
[1034/5166] Loss: 6.4746
[2068/5166] Loss: 6.1938
[3102/5166] Loss: 6.5408
[4136/5166] Loss: 6.3297
loss in epoch 78: 6.557617664337158
Epoch: 79
[0/5166] Loss: 6.3244
[1034/5166] Loss: 6.3951
[2068/5166] Loss: 6.2158
[3102/5166] Loss: 6.3880
[4136/5166] Loss: 6.4455
loss in epoch 79: 6.4296722412109375
Epoch: 80
[0/5166] Loss: 6.2408
[1034/5166] Loss: 6.1486
[2068/5166] Loss: 6.1432
[3102/5166] Loss: 6.1214
[4136/5166] Loss: 6.1706
loss in epoch 80: 5.902850151062012
start predicting:  2024-12-03 16:08:54.218308
Epoch: 81
[0/5166] Loss: 6.1617
[1034/5166] Loss: 6.2622
[2068/5166] Loss: 6.0783
[3102/5166] Loss: 6.2435
[4136/5166] Loss: 6.1857
loss in epoch 81: 6.146583080291748
Epoch: 82
[0/5166] Loss: 6.1924
[1034/5166] Loss: 6.4102
[2068/5166] Loss: 6.2133
[3102/5166] Loss: 6.3848
[4136/5166] Loss: 6.2884
loss in epoch 82: 6.647518634796143
Epoch: 83
[0/5166] Loss: 6.2749
[1034/5166] Loss: 6.3112
[2068/5166] Loss: 6.0865
[3102/5166] Loss: 6.1738
[4136/5166] Loss: 6.3869
loss in epoch 83: 5.745456695556641
Epoch: 84
[0/5166] Loss: 6.2251
[1034/5166] Loss: 6.4616
[2068/5166] Loss: 6.2585
[3102/5166] Loss: 6.0697
[4136/5166] Loss: 6.0972
loss in epoch 84: 6.068700313568115
Epoch: 85
[0/5166] Loss: 6.1235
[1034/5166] Loss: 6.0910
[2068/5166] Loss: 6.3269
[3102/5166] Loss: 6.2599
[4136/5166] Loss: 6.2671
loss in epoch 85: 6.589861869812012
Epoch: 86
[0/5166] Loss: 6.2461
[1034/5166] Loss: 6.1271
[2068/5166] Loss: 6.1505
[3102/5166] Loss: 6.2649
[4136/5166] Loss: 6.1338
loss in epoch 86: 6.146687030792236
Epoch: 87
[0/5166] Loss: 6.1106
[1034/5166] Loss: 6.0146
[2068/5166] Loss: 6.3666
[3102/5166] Loss: 5.9574
[4136/5166] Loss: 6.3596
loss in epoch 87: 6.204678058624268
Epoch: 88
[0/5166] Loss: 6.1240
[1034/5166] Loss: 6.2888
[2068/5166] Loss: 6.0741
[3102/5166] Loss: 6.1107
[4136/5166] Loss: 5.8995
loss in epoch 88: 6.0657429695129395
Epoch: 89
[0/5166] Loss: 6.2337
[1034/5166] Loss: 6.1003
[2068/5166] Loss: 6.0478
[3102/5166] Loss: 6.1572
[4136/5166] Loss: 6.0712
loss in epoch 89: 5.694430828094482
Epoch: 90
[0/5166] Loss: 6.2248
[1034/5166] Loss: 5.9619
[2068/5166] Loss: 6.1250
[3102/5166] Loss: 6.0758
[4136/5166] Loss: 6.0274
loss in epoch 90: 6.529825210571289
Epoch: 91
[0/5166] Loss: 6.1325
[1034/5166] Loss: 6.1490
[2068/5166] Loss: 6.1019
[3102/5166] Loss: 6.2757
[4136/5166] Loss: 6.0773
loss in epoch 91: 6.105988502502441
Epoch: 92
[0/5166] Loss: 5.9231
[1034/5166] Loss: 5.9738
[2068/5166] Loss: 6.0746
[3102/5166] Loss: 6.2655
[4136/5166] Loss: 6.1158
loss in epoch 92: 6.319269180297852
Epoch: 93
[0/5166] Loss: 6.2897
[1034/5166] Loss: 5.7597
[2068/5166] Loss: 6.1749
[3102/5166] Loss: 5.9758
[4136/5166] Loss: 5.8769
loss in epoch 93: 6.023092269897461
Epoch: 94
[0/5166] Loss: 6.1985
[1034/5166] Loss: 6.2463
[2068/5166] Loss: 5.8888
[3102/5166] Loss: 6.1339
[4136/5166] Loss: 6.0407
loss in epoch 94: 5.902728080749512
Epoch: 95
[0/5166] Loss: 6.0411
[1034/5166] Loss: 5.8906
[2068/5166] Loss: 6.1062
[3102/5166] Loss: 6.0015
[4136/5166] Loss: 6.1693
loss in epoch 95: 6.220621109008789
Epoch: 96
[0/5166] Loss: 6.4882
[1034/5166] Loss: 6.1580
[2068/5166] Loss: 6.1839
[3102/5166] Loss: 6.1242
[4136/5166] Loss: 6.1483
loss in epoch 96: 6.428769111633301
Epoch: 97
[0/5166] Loss: 6.0912
[1034/5166] Loss: 6.1946
[2068/5166] Loss: 5.9044
[3102/5166] Loss: 6.0013
[4136/5166] Loss: 5.9649
loss in epoch 97: 6.07421875
Epoch: 98
[0/5166] Loss: 6.0134
[1034/5166] Loss: 5.9035
[2068/5166] Loss: 6.1469
[3102/5166] Loss: 6.1941
[4136/5166] Loss: 6.2085
loss in epoch 98: 6.0919270515441895
Epoch: 99
[0/5166] Loss: 6.1403
[1034/5166] Loss: 6.0012
[2068/5166] Loss: 6.0749
[3102/5166] Loss: 5.9935
[4136/5166] Loss: 6.2280
loss in epoch 99: 5.914768218994141
Epoch: 100
[0/5166] Loss: 6.2192
[1034/5166] Loss: 6.1423
[2068/5166] Loss: 5.9612
[3102/5166] Loss: 6.1508
[4136/5166] Loss: 6.1686
loss in epoch 100: 6.023568630218506
start predicting:  2024-12-04 09:00:28.925112
Epoch: 101
[0/5166] Loss: 5.9398
[1034/5166] Loss: 5.9685
[2068/5166] Loss: 5.7510
[3102/5166] Loss: 5.7733
[4136/5166] Loss: 5.5787
loss in epoch 101: 6.119144439697266
Epoch: 102
[0/5166] Loss: 6.0239
[1034/5166] Loss: 5.7877
[2068/5166] Loss: 5.8007
[3102/5166] Loss: 5.8836
[4136/5166] Loss: 6.0660
loss in epoch 102: 6.017964839935303
Epoch: 103
[0/5166] Loss: 6.0218
[1034/5166] Loss: 5.9037
[2068/5166] Loss: 6.1467
[3102/5166] Loss: 6.1752
[4136/5166] Loss: 6.1587
loss in epoch 103: 6.006565093994141
Epoch: 104
[0/5166] Loss: 5.8134
[1034/5166] Loss: 5.7506
[2068/5166] Loss: 5.9689
[3102/5166] Loss: 5.8934
[4136/5166] Loss: 5.7745
loss in epoch 104: 6.182290554046631
Epoch: 105
[0/5166] Loss: 5.9090
[1034/5166] Loss: 5.9046
[2068/5166] Loss: 6.0010
[3102/5166] Loss: 5.8560
[4136/5166] Loss: 6.0306
loss in epoch 105: 5.687433242797852
Epoch: 106
[0/5166] Loss: 5.9552
[1034/5166] Loss: 5.7920
[2068/5166] Loss: 5.9691
[3102/5166] Loss: 5.9385
[4136/5166] Loss: 5.8855
loss in epoch 106: 5.752016544342041
Epoch: 107
[0/5166] Loss: 5.7676
[1034/5166] Loss: 6.1190
[2068/5166] Loss: 6.0151
[3102/5166] Loss: 5.8940
[4136/5166] Loss: 5.9444
loss in epoch 107: 5.9669270515441895
Epoch: 108
[0/5166] Loss: 5.9615
[1034/5166] Loss: 5.7577
[2068/5166] Loss: 5.7259
[3102/5166] Loss: 5.7021
[4136/5166] Loss: 5.6401
loss in epoch 108: 5.997142314910889
Epoch: 109
[0/5166] Loss: 6.0046
[1034/5166] Loss: 5.7069
[2068/5166] Loss: 5.8322
[3102/5166] Loss: 6.0106
[4136/5166] Loss: 5.7969
loss in epoch 109: 5.897427082061768
Epoch: 110
[0/5166] Loss: 5.9307
[1034/5166] Loss: 5.8394
[2068/5166] Loss: 5.9916
[3102/5166] Loss: 6.0655
[4136/5166] Loss: 5.7298
loss in epoch 110: 5.8198561668396
Epoch: 111
[0/5166] Loss: 5.8621
[1034/5166] Loss: 5.9155
[2068/5166] Loss: 5.7297
[3102/5166] Loss: 5.8162
[4136/5166] Loss: 5.7970
loss in epoch 111: 5.7105326652526855
Epoch: 112
[0/5166] Loss: 5.8279
[1034/5166] Loss: 5.7624
[2068/5166] Loss: 6.0811
[3102/5166] Loss: 5.7890
[4136/5166] Loss: 6.0531
loss in epoch 112: 5.766698360443115
Epoch: 113
[0/5166] Loss: 6.0561
[1034/5166] Loss: 5.8624
[2068/5166] Loss: 5.5828
[3102/5166] Loss: 6.0193
[4136/5166] Loss: 5.8508
loss in epoch 113: 6.080978870391846
Epoch: 114
[0/5166] Loss: 5.6550
[1034/5166] Loss: 5.6176
[2068/5166] Loss: 5.7684
[3102/5166] Loss: 5.7967
[4136/5166] Loss: 5.7106
loss in epoch 114: 5.879166126251221
Epoch: 115
[0/5166] Loss: 5.6903
[1034/5166] Loss: 5.8058
[2068/5166] Loss: 5.6525
[3102/5166] Loss: 5.8786
[4136/5166] Loss: 5.8416
loss in epoch 115: 6.09219217300415
Epoch: 116
[0/5166] Loss: 5.9403
[1034/5166] Loss: 5.9990
[2068/5166] Loss: 5.9934
[3102/5166] Loss: 5.7787
[4136/5166] Loss: 5.6891
loss in epoch 116: 5.63463020324707
Epoch: 117
[0/5166] Loss: 6.0144
[1034/5166] Loss: 5.7931
[2068/5166] Loss: 5.5855
[3102/5166] Loss: 5.7025
[4136/5166] Loss: 5.7348
loss in epoch 117: 5.989952564239502
Epoch: 118
[0/5166] Loss: 5.7748
[1034/5166] Loss: 5.8036
[2068/5166] Loss: 5.6898
[3102/5166] Loss: 5.7972
[4136/5166] Loss: 5.7951
loss in epoch 118: 6.098672389984131
Epoch: 119
[0/5166] Loss: 5.9414
[1034/5166] Loss: 5.9643
[2068/5166] Loss: 5.9770
[3102/5166] Loss: 5.8631
[4136/5166] Loss: 5.9523
loss in epoch 119: 6.185173988342285
Epoch: 120
[0/5166] Loss: 5.8620
[1034/5166] Loss: 5.8003
[2068/5166] Loss: 5.7581
[3102/5166] Loss: 5.8700
[4136/5166] Loss: 6.1440
loss in epoch 120: 5.889163017272949
start predicting:  2024-12-05 04:54:25.612649
Test------------------------------------------------------
{'HR@5': 0.4819, 'NDCG@5': 0.3142, 'HR@10': 0.804, 'NDCG@10': 0.4176, 'HR@20': 1.3161, 'NDCG@20': 0.5459}
Best Eval---------------------------------------------------------
{'Best_HR@5': 0.5538, 'Best_NDCG@5': 0.3667, 'Best_HR@10': 0.9127, 'Best_NDCG@10': 0.482, 'Best_HR@20': 1.5153, 'Best_NDCG@20': 0.6328}
{'Best_epoch_HR@5': 20, 'Best_epoch_NDCG@5': 20, 'Best_epoch_HR@10': 20, 'Best_epoch_NDCG@10': 20, 'Best_epoch_HR@20': 20, 'Best_epoch_NDCG@20': 20}
Namespace(batch_size=512, dataset='steam', decay_step=100, description='Diffu_norm_score', device='cuda', diffusion_steps=32, diversity_measure=False, dropout=0.1, emb_dropout=0.3, epoch_time_avg=False, epochs=500, eval_interval=20, gamma=0.1, hidden_act='gelu', hidden_size=128, item_num=13044, lambda_uncertainty=0.001, log_file='log/', long_head=False, loss_lambda=0.001, lr=0.001, max_len=50, metric_ks=[5, 10, 20], momentum=None, noise_schedule='trunc_lin', num_blocks=4, num_gpu=1, optimizer='Adam', patience=5, random_seed=1997, rescale_timesteps=True, schedule_sampler_name='lossaware', weight_decay=0)
